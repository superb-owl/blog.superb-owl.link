<html><head>
    <title>The Future of Art is Artificial - Superb Owl</title>
    <link rel="stylesheet" href="/styles.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover">
    <script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
    <script>
      fetch("https://dgftewzs2q3y4ufp7uozfqty6a0artie.lambda-url.us-east-1.on.aws/");
    </script>
    <script src="/main.js"></script>
  
        <meta name="author" content="Max Goodbird">
        <meta property="og:type" content="article">
        <link rel="alternate" type="application/rss+xml" href="https://superbowl.substack.com/feed" title="Superb Owl">

        <link rel="canonical" href="https://blog.superb-owl.link/p/the-future-of-art-is-artificial">
        <meta property="og:url" content="https://blog.superb-owl.link/p/the-future-of-art-is-artificial">
        <meta property="og:title" content="The Future of Art is Artificial">
        <meta name="twitter:title" content="The Future of Art is Artificial">
        <meta name="description" content="AI will enable creative people to do amazing things">
        <meta property="og:description" content="AI will enable creative people to do amazing things">
        <meta name="twitter:description" content="AI will enable creative people to do amazing things">
        <meta property="og:image" content="https://substackcdn.com/image/fetch/w_600,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F23c7c267-16e5-4f5d-8929-2a43785f5097_1124x556.png">
        <meta name="twitter:image" content="https://substackcdn.com/image/fetch/w_600,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F23c7c267-16e5-4f5d-8929-2a43785f5097_1124x556.png">
        <meta name="twitter:card" content="summary_large_image">
  </head>
  <body class="post-view">
    <div id="bodycontent">
      <div id="heading">
        <div class="floating-logo">
          <a href="/"><img src="/logo.png"></a>
          <a id="color-mode" onclick="toggleColorMode()"></a>
        </div>
        <h1 id="superb-owl"><a href="/">Superb Owl</a></h1>
        <div class="subheading small linkbar">
          <a target="_blank" href="https://superbowl.substack.com/about">About</a>
          |
          <a target="_blank" href="https://superbowl.substack.com/feed">RSS</a>
          |
          <a target="_blank" href="https://superbowl.substack.com/">Substack</a>
          |
          <a target="_blank" href="https://twitter.com/owlthatissuperb">Twitter</a>
        </div>
        <span class="subheading small">Subscribe via Substack:</span>
        <br>
        <iframe class="subscribe-embed" src="https://superbowl.substack.com/embed" frameborder="0" scrolling="no"></iframe>
      </div>
      <div id="posts"></div>
      <div id="post">
    <hr class="post-title-top">
    <h1 class="post-title" id="the-future-of-art-is-artificial">The Future of Art is Artificial</h1>
    <h3 class="post-subtitle" id="ai-will-enable-creative-people-to-do-amazing-things">AI will enable creative people to do amazing things</h3>
    <a class="substack-link" href="https://superbowl.substack.com/p/the-future-of-art-is-artificial">View on Substack</a>
    <hr class="post-title-bottom">
    <p>AI-generated media has quite suddenly leapt from <a href="https://en.wikipedia.org/wiki/DeepDream">quirky</a> and error-prone to <a href="https://dallery.gallery/">novel and compelling</a>. With the <a href="https://stability.ai/blog/stable-diffusion-public-release">public release of Stable Diffusion</a>, I think it’s worth taking a look at how <strong>AI is going to invigorate creative professions in the coming years</strong>. </p><p>When a new technology emerges, it’s natural to lament the disruption and loss of skills it will inevitably bring. But there are also plenty of reasons to be excited: <strong>AI will enable a new tranche of creators to begin expressing themselves</strong>, and allow innovative professionals to create experiences that would have previously been impossible.</p><p>I think <strong>we’re in the first inning of this transformation</strong>. Let’s look at ways AI might enhance our creativity.</p><h2 id="outline">Outline</h2><ul><li><p>Principles of Creation</p><ul><li><p>Creation vs Discovery</p></li><li><p>Creative Technologies</p></li></ul></li><li><p>Exploring Vector Spaces with AI</p><ul><li><p>Autoencoders</p></li><li><p>Style Transfer</p></li><li><p>Fill-in-the-Blank</p></li></ul></li><li><p>Human-Computer Interfaces</p><ul><li><p>Iterative Prompts</p></li><li><p>Domain Modeling</p></li><li><p>Compute</p></li></ul></li><li><p>Transforming Media</p></li></ul><div class="subscription-widget-wrap"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption"></p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email"><input class="button primary" type="submit" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h1 id="principles-of-creation">Principles of Creation</h1><p>Before we get started, we’ll need to talk a bit about the creative process, so we can see how AI fits in.</p><h2 id="creation-vs-discovery">Creation vs Discovery</h2><blockquote><p>The sculpture is already complete within the marble block, before I start my work. It is already there, I just have to chisel away the superfluous material.</p><p>- <a href="https://quoteinvestigator.com/2014/06/22/chip-away/">not</a> Michelangelo</p></blockquote><p><strong>There’s a fuzzy boundary between creation and discovery.</strong> Were computers created or discovered? To me, the abstract concept of <a href="https://en.wikipedia.org/wiki/Turing_machine">turing machines</a> exists in some Platonic mathematical space, waiting to be discovered; <a href="https://en.wikipedia.org/wiki/Transistor">silicon transistors</a>, on the other hand, seem like a fairly arbitrary configuration of matter that had to be invented, i.e. created. But it’s hard to justify this distinction on first principles.</p><p>For a real mindfuck, consider the fact that <strong>there are only a finite number of images that can fit on your computer screen</strong>. A two-line computer program can enumerate every possible image, independently creating the Mona Lisa, every photo of your kids, every Looney Tunes cel, literally everything. You’ll often hear this referred to as “image-space,” or more abstractly, a “vector space.”</p><p><strong>A creator’s job is to explore this space.</strong></p><p>Of course, <strong>it’s a </strong><em><strong>huge</strong></em><strong> space</strong>. Even with strict constraints, like only black-and-white 1 megapixel images, would have something like 2^1000000 images, which Duck Duck Go tells me is equal to infinity. There’s probably not enough time left in the universe for our program to finish.</p><p>It’s also a space filled with mostly boring images. Pick a spot at random, and you’re almost guaranteed to see static.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F24f15811-1cfe-48f9-862c-f002bc146a91_1548x494.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="Duck duck go search for 2^1000000, with the result Infinity" class="sizing-normal" height="465" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F24f15811-1cfe-48f9-862c-f002bc146a91_1548x494.png" title="Duck duck go search for 2^1000000, with the result Infinity" width="1456"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Seems about right.</figcaption></figure></div><p>But under the hood, <strong>all creation is an act of discovery</strong>. When we say that painting is a <em>creative</em> endeavor, we mean that you can’t just bumble around in image-space hoping to discover an island of beauty. You have to have a clear vision for where you’re going and how you’re going to get there. Non-creative discovery is much easier—Christopher Columbus just had to point west and go forward.</p><p>The same goes for novels, music, movies, video games, etc. <strong>Human experience is bounded by finite time, space, and resolution</strong>, but there’s still a heck of a lot to explore.</p><h2 id="creative-technologies">Creative Technologies</h2><p>Now that we’re used to thinking of art in terms of vector spaces, we can talk about how tech fits in.</p><p>Creative technology helps by making exploration easier—<strong>it makes the act of creation feel more like discovery</strong>. There are thousands of ways to help navigate huge spaces, but here are a few common patterns:</p><ul><li><p>You can bound the space, weeding out a lot of static</p><ul><li><p>E.g. tonal instruments provide a specific set of curated notes, instead of the full range of frequencies you can sing</p></li></ul></li><li><p>You can project a large space onto a smaller space, which can be more easily explored</p><ul><li><p>E.g. an arpeggiator can take a single frequency and generate a much more complex sound</p></li></ul></li><li><p>You can enable the exploration of nearby points in space, once you’ve found a good one</p><ul><li><p>E.g. Photoshop can take an image and produce millions of “nearby” images, some of which look better</p></li></ul></li></ul><p>The wonderful thing about AI is that <strong>it’s </strong><em><strong>really good</strong></em><strong> at exploring gigantic vector spaces</strong>. That’s literally all Deep Learning is—finding ways to explore, map, and constrain very large vector spaces.</p><h1 id="exploring-vector-spaces-with-ai">Exploring Vector Spaces with AI</h1><p>If we’re going to talk about exploring vector spaces, there are a few AI-related concepts you should know about.</p><h2 id="autoencoders">Autoencoders</h2><p>Autoencoders are automatic compression algorithms. They can <strong>take a giant space, like our image-space above, and cut it down to something much smaller</strong>.</p><p>Of course, there’s no free lunch when it comes to compression. An autoencoder has to target a particular subspace, like pictures of hotdogs or handwritten numbers. If the autoencoder is successful, <strong>you get a really nice low-dimensional space for exploring your domain</strong>.</p><p>Here’s an <a href="https://arxiv.org/pdf/1511.05644.pdf">amazing paper</a> that creates a 2D space for exploring handwritten digits using an autoencoder. The results are beautiful:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F23c7c267-16e5-4f5d-8929-2a43785f5097_1124x556.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="" class="sizing-normal" height="556" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F23c7c267-16e5-4f5d-8929-2a43785f5097_1124x556.png" width="1124"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Autoencoder that maps handwritten digits onto a spiral or a star in 2-d space</figcaption></figure></div><p>In the example above, each point on the plane corresponds to an image; blue dots correspond to images that are labeled “0,”  green dots correspond to “1”s, etc. The points in the corners, far away from any color, probably represent static.</p><p>This covers two of our strategies for exploring a large vector space:</p><ul><li><p>It projects a large space onto a much smaller (i.e. lower dimensional) space, which is easier to explore</p></li><li><p>It makes it easy to take an image and explore nearby images—e.g. I can draw a “2,” find its spot in the plane, and move around a bit to find similar-looking “2”s.</p></li></ul><h2 id="style-transfer">Style Transfer</h2><p>Style transfer <strong>constellates a small volume in your vector space</strong>, and allows you to map all sorts of things onto it.</p><p>For example, we might start by defining a tiny corner of image-space, like “Vincent van Gogh paintings.” Style transfer allows us to ask questions like, “What if van Gogh painted the Mona Lisa?”</p><p>Hypothetically we can give our style transfer algorithm <em>any</em> image, and it will map it onto the comparably tiny space of “Vincent Van Gogh paintings.”</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F83a9c10b-5e7a-4ecb-a732-efe4850c54af_483x720.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="" class="sizing-normal" height="720" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F83a9c10b-5e7a-4ecb-a732-efe4850c54af_483x720.jpeg" width="483"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Note that <strong>this isn’t limited to imitating individual artists</strong>. You could pick a specific era, brush stroke technique, color composition, whatever. You just have to give the algorithm a set of target images that define your domain, and it’ll do it’s best to map other images nearby.</p><h2 id="fill-in-the-blank">Fill-in-the-Blank</h2><p>Fill-in-the-blank problems define a volume in the vector space, and <strong>ask the AI to find “likely” points</strong> within that volume. For example, we might give an AI algorithm an image with a piece missing, and let it guess at how to fill it in:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F876c9e16-81cc-4bcb-ab1a-c91b40084b54_1326x672.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="" class="sizing-normal" height="672" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F876c9e16-81cc-4bcb-ab1a-c91b40084b54_1326x672.png" width="1326"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Credit to <a href="https://openai.com/blog/image-gpt/">OpenAI</a></figcaption></figure></div><p>What’s “likely” is determined by the other data the algorithm has been trained on. If it’s only ever seen pictures of cats, it will probably try and put some whiskers in there.</p><div class="subscription-widget-wrap"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption"></p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email"><input class="button primary" type="submit" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h1 id="human-computer-interfaces">Human-Computer Interfaces</h1><p>So where’s the low-hanging fruit in building AI for creative professionals? Most of it comes down to understanding natural language.</p><p><strong>Natural language is the holy grail of interfaces</strong>. Currently we have to bend to our machines—we speak to them hunched over keyboards, dragging cursors around on screens, interacting in an esoteric and unforgiving “language.” You clicked the wrong button? Sorry, you’ll need to fill out the entire form again.</p><p>Perfect, <strong>human-level natural language understanding will feel like having an army of semi-skilled interns</strong> in your pocket. But we’ll need a few things before we get there.</p><h2 id="iterative-prompts">Iterative Prompts</h2><p><strong>The latest algorithms</strong> <strong>can understand single, specific commands</strong> really well. “A pencil drawing of an astronaut riding a horse” or “Who won the World Series in 1995?” or “How do I get to Carnegie Hall?” are all well within reach for contemporary algorithms.</p><p>But <strong>they’re still lacking when it comes to iterative tasks</strong>. I can’t ask for my pencil drawing of an astronaut riding a horse, then ask for a bigger horse, fewer pencil marks, oh and please add Saturn in the background.</p><p>We <em>could</em> construct one giant prompt with all of this information, but that’s not as helpful. Each time I go back to my prompt to add more information, the previous prompt is forgotten, as well as the image it generated. Maybe I was very happy with my astronaut, and just wanted a slightly bigger horse.</p><p>DALL-E can <em>kind of</em> do this. You can erase an area of your picture, and rephrase your entire prompt. The results aren’t terrible, but also not great. Currently, the creative loop consists of prompt → photo editing → prompt → photo editing → prompt, instead of just prompt → prompt → prompt.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0428b29b-1bde-4e05-b15f-b7d31a1fab14_1248x1052.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="" class="sizing-normal" height="1052" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0428b29b-1bde-4e05-b15f-b7d31a1fab14_1248x1052.png" width="1248"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">This blog’s logo (originally a DALL-E creation), with “Saturn in the background” added to the prompt</figcaption></figure></div><p>An iterative method allows us to quickly find a desired volume in the vastness of image-space (e.g. astronauts riding horses) and then slowly explore that area, honing in on exactly what we want. If instead I’m forced to revise my initial prompt, the AI starts again from scratch, and we end up with a big jump in image-space, which is jarring.</p><p><strong>The next wave of AI-based creativity will need to see huge progress in iterative prompts</strong>. That will enable an entire <em>process</em> to develop around AI-generated art, and it will feel more like a collaborative exercise, as opposed to the one-off call and response we get today.</p><h2 id="domain-modeling">Domain Modeling</h2><p>A big part of <strong>improving iterative prompting will mean deeper domain-specific understanding</strong>. The prompt “stronger” means something very different for a picture of a man, a picture of a bridge, a speech about human rights, or a guitar riff.</p><p>DALL-E and GPT-3 seem to do an incredible amount of domain modeling already, just from having seen lots of data. <strong>It’s an open question whether engineers will need to hack in a little domain knowledge</strong>, or if further advances will just need more data. I lean towards the former, but I’ve <em>always</em> leaned in that direction, only to be surprised by a general model working on a huge dataset.</p><p>A good example here is text in DALL-E-generated images.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a566859-3b5f-4a5f-8566-ff97b1097376_2458x1042.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="DALL-E results for the prompt &quot;An album cover for the band Funkadelic&quot;" class="sizing-normal" height="617" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a566859-3b5f-4a5f-8566-ff97b1097376_2458x1042.png" title="DALL-E results for the prompt &quot;An album cover for the band Funkadelic&quot;" width="1456"><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="16" stroke="#FFFFFF" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="16" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>There’s some amazing domain understanding here! Tell me that’s not George Clinton with the big rainbow hat.</p><p>But DALL-E completely flubs the text. It draws mostly English-looking letters, and even reasonable syllables. It even seems to get that the band name should start with an “F”. But I doubt even Funkadelic would release an album called “Fackel Findunle.”</p><p>There’s enough progress here that it’s tempting to think we just need more data. But surely it’d be easier to wire in GPT-3 to bring in a huge amount of text-domain knowledge?</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1adf92f-9227-455d-90a2-effc8284c431_1544x264.png" target="_blank"><div class="image2-inset"><source type="image/webp"><img alt="GPT-3 prompt &quot;Suggest a name for the next Funkadelic album,&quot; with response &quot;Funkadelic Forever&quot;" class="sizing-normal" height="249" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1adf92f-9227-455d-90a2-effc8284c431_1544x264.png" title="GPT-3 prompt &quot;Suggest a name for the next Funkadelic album,&quot; with response &quot;Funkadelic Forever&quot;" width="1456"><div></div></div></a></figure></div><p>The words “wire-in” are doing a lot of work there, but you see where I’m going. Trying to re-learn English by looking only at text embedded in images is probably <em>doable</em>, but it’s not the most elegant solution to the problem.</p><h2 id="compute">Compute</h2><p>Finally, if we’re going to make AI a part of the creative process, <strong>we need it to be faster and cheaper</strong>.</p><p>Currently it takes about 10-15 seconds to generate an image with DALL-E, and costs about $0.13 per prompt. For basic tasks like generating a logo for your blog, this is perfectly reasonable.</p><p>But <strong>for ambitious art projects, this could be prohibitively expensive</strong> and time-consuming, especially if we start thinking about video. An hour of footage, at 30 FPS, would cost around $14,000 and take 300 hours of compute. And that’s just for the final production! Never mind all the frames that get generated and thrown out during the creative process. The true cost would easily be an order of magnitude higher.</p><p>To fully enable AI-centric creation, we’ll need rapid feedback—say, 1-2 seconds per prompt. And we’ll need the ability to explore and prototype without worrying about the cost. Better <strong>ways to deploy these models on local hardware would be a huge help</strong> here, since it would eliminate the marginal cost of each prompt, and artists could scale up their rig for faster generation. This is why I’m so excited for the <a href="https://stability.ai/blog/stable-diffusion-public-release">public release of Stable Diffusion</a>.</p><h1 id="transforming-media">Transforming Media</h1><p>I wanted to talk a lot more about how AI-based tools will transform specific industries, like art, television, music, and writing. But this is already a long article and it’s late. If you’re interested in reading part two, you know what to do.</p><div class="subscription-widget-wrap"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption"></p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email"><input class="button primary" type="submit" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h2 id="related">Related</h2><div class="embedded-post-wrap"><a class="embedded-post" href="https://superbowl.substack.com/p/links-for-september?utm_source=substack&amp;utm_campaign=post_embed&amp;utm_medium=web"><div class="embedded-post-header"><img class="embedded-post-publication-logo" src="https://substackcdn.com/image/fetch/w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2dc69925-91e5-4348-a20a-21dccd7c0f28_1024x1024.png"><span class="embedded-post-publication-name">Superb Owl</span></div><div class="embedded-post-title-wrapper"><div class="embedded-post-title">Links for September</div></div><div class="embedded-post-body">The Century of the Self The Century of Self is a 2002 documentary from legendary filmmaker Adam Curtis, tracking the rise of the post-WW2 advertising industry…</div><div class="embedded-post-cta-wrapper"><span class="embedded-post-cta">Read more</span></div><div class="embedded-post-meta">5 months ago · 1 like · 2 comments · Superb Owl</div></a></div>
    
    <hr>
    <a href="https://superbowl.substack.com/p/the-future-of-art-is-artificial/comments">
      Join the discussion on Substack!
    </a>
  </div>
    </div>
    <script src="/main.js"></script>
  

</body></html>